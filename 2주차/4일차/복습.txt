교차검증
    학습-평가 data를 골고루 설정하여 모델의 안정성을 높이고 과대적합을 감소시키는 통계적 기법
    같은 data 내에서 학습, 평가 data를 나누기 때문에 검증에 충분하지 않다

    holdout
        학습, 검증, 평가 data를 각각 만들어 2번 검증

    k-fold
        1개의 data set을 k개로 나눠 k-1개 data set을 학습시킴 -> 나머지 1개의 data set으로 평가 -> k번 수행 -> 각 평가결과의 평균 구함

    장단점
        모든 data를 사용하기 때문에 과대적합 감소 -> 계산량 증가 -> 시간, 비용 증가
        모델의 data의 변경에 대한 민감도 파악 가능
    
grid search
    hyper parameter 설정은 모델링에서 매우 중요
    관계있는 hyper parameter들을 대상으로 가능한 모든 조합을 시도
        -> dict 형태로 입력


비지도학습
    구조를 가정하고 data를 구조에 맞추어 설명하는 방법
    정답 값이 주어지지 않은 입력 data들을 분석하여 공통점이 있는 data끼리 묶어 그룹핑하는 방식
        -> x_train만 제공
    
    종류
        차원축소 : PCA, t-SNE, NMF
            X-1 차원으로 재축소
        군집 : K-Means, Hierarchial, DBSCAN
    
    장단점
        학습결과를 평가하기 어렵다 -> 묶인 군집 data들을 다시 한번 확인해야함
        EDA, 전처리에서 활용하기도 함
        scale 조정 메서드도 지도 정보(supervised information)을 사용하지 않으므로 비지도 방식
            scaler -> x_train data만 입력해줌

    차원축소
        PCA
            text -> 잠재의미분석
            이미지 -> 고유이미지

            1) 분산이 가장 큰 방향(벡터)를 찾음
            2) 찾은 벡터와 직교하면서 가장 큰 정보를 담은 벡터를 찾음(n차원에서 n개)
            3) 주성분을 구한 후 기존 data에서 평균을 구함 -> 기존 data로 돌려보냄

            분산에 대한 정보 유지, 정보 손실 적음, 선형 data에 적합

            pca.fit(x_train)
            pca.transform(x_test)
            -> x_train에 대해 학습시켜도 x_test data를 변환할 수 있음

        t-SNE
            데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾는 것 -> 3개 이상 변수를 가지지 않음
            x data를 넣어주면 x에 대해서만 작동
            다른 data를 넣어주려면 다시 fit과 transform을 해줘야함
            가까운 포인트를 유지하는게 더 큰 목적 -> 이후 포인트에 대한 보정    
            비선형 data에 적합, 시각화에만 도움

        장점
            다중공선성 해결 가능 -> feature 간 독립
            많은 정보를 효과적으로 시각화해서 data 특성 탐색 가능
            연산 속도 향상

        단점
            정보 손실 발생
            결과 data 해석하는데 어려움이 있음