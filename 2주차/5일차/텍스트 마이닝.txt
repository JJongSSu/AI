텍스트 마이닝
    자연어 처리방식, 문서처리 방법
    자연어 : 모든 언어 <-> 인공언어 : 특정 목적을 위해 인위적, 의도적으로 만든 언어

    인간의  언어가 사용되는 실세계 모든영역
        - 정보검색, 질의응답
            구글, 네이버, siri, bixby, IBM Watson
        - 기계번역, 자동통역
            구글번역, 네이버 파파고, ETRI 지니톡
        - 문서작성, 문서처리 등
    
    활용사례
        1. 지식경영 : 의미있는 data만 뽑아냄
        2. 사이버 범죄 예방 : 특정 단어 분류 -> 범죄 예측, 스팸 분류
        3. 고객 관리 서비스 : 자동화된 응답 제공 -> 챗봇
        4. 콘텐츠 요약 : 회의록 작성
        5. 소셜 미디어 데이터 분석 : 다양한 의견과 감성반응 살핌 -> 리뷰 data 크롤링
    
    텍스트 분류, 감성분석, 텍스트 요약, 군집화 및 유사도 분석

    말뭉치 > 문서 > 문단 > 문장 > 단어 > 형태소
        하위 단계의 집합

    분석 프로세스
        텍스트 수집 -> 전처리 -> 토큰화 -> 특징 추출(임베딩) -> data 분석
            - 텍스트 수집 : 웹 크롤링, 빅카인즈 뉴스 data, NDSL 논문 data
            - 전처리 : 오탈자 제거, 띄어쓰기, 불용어(의미없는 단어) 제거, 정제(노이즈 data : 빈도가 적은 단어, 의미없는 특수문자), 
                    정규화(비슷한 의미 단어 통합), 어간 추출(단어 중 핵심부분만 추출), 표제어 추출(유사한 단어들에서 대표 단어 추출)
            - 토큰화 : 말뭉치를 나누는 작업(공백(영어 text), 형태소, 명사, 단어기준), 기준은 분석 방법에 따라 다름
                    감성 분석 -> 한글에서 감성을 나타내는 품사가 동사, 형용사에 가깝기 때문에 한글 형태소 분석기를 사용해서 동사, 형용사 위주로 추출
            - 특징 추출 : 임베딩(text data를 벡터형태로 변환), 중요한 단어 선별(적은 수의 문서에 분포, 분서 내에 빈도 많아야 함)
                        -> 모든 문서에 분포되어 있으면 차별성이 없음
            - 분석 : 머신러닝, 딥러닝
    
        토큰화 종류
            단어(word) 단위, 문자(character) 단위 
            n-gram 단위 : 여러 개의 단어를 하나의 단어로 치환, 벡터를 1개로 만들어 줌
        
        임베딩
            - 원핫인코딩 : 포함되어 있으면 1, 아니면 0
                토큰에 고유번호를 배정 -> 모든 고유번호 위치의 한 칼럼만 1, 나머지 컬럼은 0인 벡터로 표시하는 방법

            - BOW(bag of word) : CounterVectorize, TF-IDF
                - CounterVectorize : 빈도수를 기반으로 벡터화(결과값 = 각 벡터의 빈도수) 
                    -> 단어의 순서나 중요도를 고려하지 않기 때문에 문맥의 의미를 반영하기 힘듬
                - TF-IDF : 개별 문서에서 자주 등장하는 단어에 높은 가중치 부여, 모든 문서에 자주 등장하는 단어는 패널티 부여(단어의 중요도 반영)
                    TF = 단어가 각 문서에서 발생한 빈도
                    DF = 단어가 등장한 문서의 수
                    IDF = DF의 역수
                    적은 문서에서 많이 발견될수록 가치 있는 정보
                    희소성을 반영하기 위해 TF*IDF 값 사용

                    W = TF * log(N/DF)
                        N = 전체 문서의 수
                        N이 커지면 IDF가 기하급수적으로 커지는 것을 방지하기 위해 log 사용

            - Word2Vec
                단어 벡터 사이의 거리 측정
                - CBOW
                    맥락으로부터 타겟을 추측하는 신경망

                - Skip gram
                    타겟으로부터 주변 맥락을 추측하는 신경망
                    역전파 사용 -> 여러 단어의 가중치를 알 수 있음