SVM
    신경망 알고리즘보다 사용이 간결
    분류나 회귀 분석에 사용 가능하지만 분류에서 주로 사용
    *다른 레이블(클래스) 사이에 존재하는 여백(거리=margin)을 최대화하려는 목적으로 설계
    support vector와 Hiperplane을 이용해서 분류를 수행하는 알고리즘
    선형으로 분리할 수 없는 점들을 분류하기 위해 커널 사용
    커널은 더 높은 차원의 data로 변환
    RBF 커널 hyper parameter = gamma -> 너무 크면 과대적합 발생
    
    장점 : 
        오류 data에 대한 영향이 적음 -> C값(이상치 허용 정도)
        과대적합 적음
        신경망보다 사용하기 쉬움
    
    단점 : 
        파라미터 조합 성능 test가 필요함
        학습 속도느림
        블랙박스 형태 -> 모델 해석이 어렵고 복잡


Hiperplane : N차원 공간에서 N-1차원으로 나눠주는 subspace
support vector : data set 안에 포함되어 있는 두 가지 레이블로 구분되는 data 중 최외각에 있는 샘플들의 vector
                Hiperplane과 가장 가까운 vector
margin : support vector통해 구한 두 레이블 사이의 거리 -> 최대화 하는것이 목적
        Hiperplane과 support vector 사이의 거리

Hiperplane을 잘 분류하는 기준
    한쪽에 치우치지 않도록 분류
    양쪽 data와 균등한 위치에 분류 기준 세우기
    -> 각 레이블에 대해 margin 길이가 동일하게 설정

SVM 분류 방법
    1. 두 레이블 중 어느 하나에 속한 data 집합이 주어짐
    2. 주어진 data set을 바탕으로 새로운 data가 어느 레이블에 속하는지 판단하는 비확률적 선형분류모델을 만듬
    3. 만들어진 분류 모델은 data가 있는 공간에서 경계로 표현 -> 가장 큰 폭을 가진 경계를 찾음

이상치 허용 (hyper parameter = C : 높을수록 이상치 허용 안함)
    하드 margin : 이상치 허용 거의 안함, C값 높음
        경계선이 곡선형태가 됨, C값 너무 커지면 과대적합
    소프트 margin : 어느정도 이상치 허용, C값 낮음 -> 모델이 과대적합 되지 않고 어느정도 일반화된 모델을 만들겠다
        C값 너무 작아지면 과소적합

kernal SVM
    1. 선형
    2. polynomial(다항식)
        N+1차원으로 꺼내서 결정경계(Hiperplane)를 긋는 방식
    3. RBF(Radial Bias Function) : 가우시안
        무한한 차원으로 변환 -> 선형대수 사용 -> 다차원의 결정경계 긋는 방식
        hyper parameter = gamma 
            : 각 data들의 영향 범위를 조정, 값이 커지면 하나의 data가 큰 영향범위를 가짐 -> 결정경계가 곡선을 이룸, 값이 작아지면 결정경계 일반화
                gamma 너무 크면 과대적합, 너무 작으면 과소적합

        굳이 다차원으로 안만들어도 되는 경우에도 그냥 가우시안 사용??

<hyper parameter>
    C(이상치 허용)
        허용 O(소프트 margin) : margin 커짐 -> 너무 허용은 안되지만 일반적으로 허용하는 방향으로 만들어주기
        허용 X(하드 margin) : margin 작아짐

    gamma(data의 영향범위) : 결정경계 곡률 결정
        값이 커지면 과대적합, 작아지면 과소적합


