앙상블
    여러 개의 모델이 예측한 값을 결합하여 정확한 예측을 하는 기법

사용하는 이유
    1. 단일 모델에 비해 높은 성능과 신뢰성을 얻을 수 있음

    2. 여러 모델을 사용하기 때문에 data가 적은 것에 대해 학습 효과를 얻을 수 있음
        과대적합 방지

<종류>
    보팅 : 다른 종류의 모델로 여러 개의 결과를 예측하여 투표/평균을 통해 최종 선정
        하드 보팅 : 투표 -> 다수결
        소프트 보팅 : 각 확률의 평균

    배깅 : 같은 모델을 가지고 조건을 다르게 여러 개 만들어서 각 결과값을 종합하여 투표/평균을 통해 최종 결과를 내는 것
        각 모델 간 독립적으로 병렬처리, 동시에 진행
        data 값들의 편차가 클 경우 사용, 과대적합 방지
        data 무작위 선택
        랜덤 포레스트 : 여러 개의 결정 트리 모델로 예측한 값을 투표를 통해 최종선택하는 모델
            다수결, 오차 평균화
            과대적합을 통계적 방법으로 해소
            부스팅에 비해 수행속도 빠름
            모델 튜닝 시간 많이 필요 -> 변수 선정 가능-> feature importance 확인 가능
            큰 data 세트에도 잘 동작하지만 트리 개수가 많아질수록 시간이 오래 걸림
            
            단점 : 선형모델보다 예측 느림

            결정트리
                직관적 -> 결과 이해하기 쉬움 
                과대적합 되기 쉬움
            결정트리의 과대적합 보완하기 위해 랜덤 포레스트 사용

    부스팅 : 순차적 학습 모델
        잘못 예측한 data들에게 가중치를 주어 업데이트 -> 다음 모델로 넘어감 -> 다시 분류 -> 다시 data 업데이트 -> 다음 모델
        학습 정확도가 낮거나 오차가 클 경우 사용, 과소적합 방지
        data 무작위 선택(가중치 적용)
        Ada, Gradient, XG, Light GBM
        XG : early stopping(더 이상 학습효과가 없을 때 멈추기 위한 hyper parameter) 제공, 과대적합 방지


            랜덤포레스트                                부스팅 -> 트리 계열
            분류/회귀                                   분류/회귀
            Scaling 필요 없음                           Scaling 필요 없음
            비선형                                      비선형
            hyper parameter 많음                        hyper parameter 많음
장점        높은 범용성                                 높은 범용성
단점        단일 모델에 비해 시간 오래걸림               시간 오래 걸림, 과대적합