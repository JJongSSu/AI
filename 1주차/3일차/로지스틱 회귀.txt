<로지스틱 회귀(Logistic Regression)> 
분류, 기하 모델 -> 단위 중요
회귀와 분류를 동시에 사용하기 위해 로지스틱 회귀 사용
    (회귀모델을 짠다음 로스스틱 회귀를 통해 분류하도록 하는 방법)

선형모델 방식을 분류에 사용
    선형모델은 간단한 함수식을 사용하기 때문에 학습 및 예측 속도가 빠름
    큰 data set에도 잘 동작
    일반적으로 특성이 많을수록 더 잘 동작

선형회귀 직선을 통해 두 집단을 나눌 수 있다

Hyper parameter
    C : 규제강도의 역수
    max_iter : 경사하강법을 위해 반복 횟수   머신러닝 Iteration <-> 딥러닝 Epoch
    L2규제 사용, 중요한 특성이 별로 없다면 L1 사용해도 됨

Sigmoid 함수 사용 
    -> 모든 분류 값을 0~1 사이의 확률 정보로 표시 가능
    y = 1 / (1 + e^(-x))
    0.5 기준으로 낮으면 0, 높으면 1로 예측 -> 몇 %로 0인지, 1인지 판단 가능
        x=0, 0.5 : 존재하지 않음
        x=무한대, 1 : 1번 레이블
        x=-무한대, 0 : 0번 레이블
    -> 어떤 회귀선이든 0~1사이 값을 가짐, 중심은 50%로 수렴
    -> 회귀 모델을 분류모델로 만들 수 있음

    기하적 모델 : 좌표평면에 찍을 수 있다, 단위에 따라 나뉨
    확률 모델 : 빈도를 알 수 있다, 동시출현에 따라 나뉨
    -> 기하적 확률 모델
        표본집단 면적 / 모집단 면적
        선일 때는 0 -> 면적과 선의 개념이 다르기 때문에, 면은 선의 무한한 모임 -> (선/모집단면적)에서 모집단 면적이 무한대이기 때문에 0으로 수렴
        x<0, 0<y<0.5 = 0번 레이블
        x>0, 0.5<y<1 = 1번 레이블
        x=1, y=0.5 : 나올 확률 0이기 때문에 고려하지 않음 -> (점/선)에서 선은 점의 무한한 모임이기 때문에 0으로 수렴
        -> 따라서 2진법 외에 사용할 수 없다

단점 : 2진 분류, 레이블이 2개일 때만 사용 가능
    2개를 넘어가면 레이블의 범위가 달라지기 때문에 공평하지 않음
    양쪽 끝은 무한대로 수렴하기 때문
    선형을 전제 만족
    

보완방법(다진분류)
    <lib 사용 가능>
    - Softmax
        (ei)^x / ((e1)^x+(e2)^x+(e3)^x+...+(ek)^x)
        e = 레이블의 개수
        i = 해당 레이블
        해당 레이블에 할당될 확률을 구하는 방법, 0~1 사이값
    
    <직접코딩>
    - OVO
        이진분류 model을 여러 개 만들어 리그형식으로 매치시키는 방법
        중복이 되면 많이 나온 결과값으로 결정(A,A,B -> A)
            model 1 = A vs B
            model 2 = B vs C
            model 1 = A vs C
        Softmax보다 성능이 좋음
        분류모델은 레이블 개수가 많아질수록 성능이 떨어짐
        여러 개의 model을 돌려야 되기 때문에 시간이 오래걸림

    - OVR
        여집합 사용
        먼저 model 1을 돌려 A를 뽑아내고 나머지 여집합 중에서 다시 model 2를 돌려 B를 뽑아내는 방식
            model 1 = A vs A^c
            model 2 = B vs B^c
        OVO보다 계산 시간 빠름, 결과가 겹치지 않음
        data 불균형이 발생함 -> model이 한쪽으로 밀어버릴 수 있음
            (A B C가 각각 30 30 30이면 30:60이 되기 때문)


손실함수 : 최적화 척도 판별하는 기준

딥러닝 관성(momentum) -> 허들을 넘어보려고 설계하는 방식
경사하강법은 mse 최저값을 찾는 방법인데 처음 최저값이 나오면 수렴하려는 경향이 있음 -> 그래서 다음 최저값을 찾고싶을 때는 딥러닝 관성 사용
경사하강법은 랜덤하게 시작
4차함수 넘어가면 local minimum이 여러 개일 수 있음
