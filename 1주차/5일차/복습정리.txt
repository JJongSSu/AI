                                머신러닝
            지도학습    비지도학습      강화학습    오토인코더
레이블          O           X           보상        input
          분류    회귀
레이블    범주    연속
평가기준  정확도   mse  

기하모델 : 모든 data를 좌표평면에 표기, scaling 필요
    선형 : x축으로 y를 정의 가능
    비선형

확률모델 : 동시출현, scaling 필요 X

<모델 종류>
1. KNN
    지도학습, 분류, 기하, 비선형
    hyper parameter = k(이웃 개수)
    장점 : 학습 빠름, 직관적
    단점 : 예측속도 느림 -> lazy algorithm

2. 선형회귀
    지도, 회귀, 기하, 선형
    hyper parameter = 없음
    parameter = 기울기, y절편
    장점 : 직관적, 대중적
    단점 : 과대적합 발생 시 조율할 방법이 없음, 
        선형전제(선형성, 독립성 : 다른 요인에 의해서 영향을 받으면 안됨(그래프의 규칙성 확인), 등분산성 : 분산정도, 정규성 : 모든 구간에서 일정)

    1) Ridge : mse + (베타)^2, 특정 변수 유지
    2) Lasso : mse + |베타|, 특정 변수 삭제
        지도, 회귀, 기하, 선형
        hyper parameter = alpha(규제 강도)
        장점 : 직관적, 조건(선형전제) 맞으면 성능 좋음
        단점 : 선형전제

3. 로지스틱 회귀
    지도, 분류, 기하, 선형
    hyper parameter = C(규제 강도), max_iter(경사하강법을 수렴시키기 위한 반복학습 횟수)
    sigmoid 사용
    장점 : 직관적, 회귀모델과 호환 가능, 조건 맞으면 강력
    단점 : 선형전제, 2진분류만 가능

    - 다진분류
        softmax : sigmoid 방식 사용
        OVO : 리그형식
        OVR : 여집합 이용

4. 의사결정 나무
    지도, 분류/회귀, 확률
    hyper parameter = max_depth, mas_leaf_nodes...
    장점 : fitting 할 수 있는 hyper parameter가 많음(조율할 수 있는게 많음), 조건 필요 없음, scaling 필요 없음
            -> input data가 숫자이기만 하면 가능 
    단점 : 최적화하기 귀찮음, 과대적합 심함(한번 잘못 들어가면 해결할 방법이 없음)

    과대적합 보완방법
        1) 보팅(voting) : 서로 다른 모델이 예측 후 투표
            hard voting : 다수결
            soft voting : 주주총회(확률)
        2) 배깅(bagging) : 하나의 모델에서 hyper parameter만 다른 모델들을 랜덤으로 만들고 예측 후 투표, 분산 줄임, 과대적합 대비
            랜덤 포레스트 : 의사결정 나무 개수를 정해서 각각 예측 후 투표
        3) 부스팅(boosting) : 경사하강법처럼 이전에 학습한 hyper parameter과 새로운 hyper parameter를 비교하며 최적의 hyper parameter를 찾아냄, 
                                2번 공부, 과소적합 대비
            Ada, Gradient, XG, Light GBM


배깅+부스팅
OVR : data불균형 해결

품질
    기본품질 : 제품으로써 인정받을 수 있는 최소한의 조건, 많아도 효과는 없지만 없으면 평가가 낮아짐
    성능품질 : 기대충족에 따른 평가가 올라감
    감동품질 : 기대하지 않은 걸 제공받았을 때 효과가 큼, 제공하지 않아도 문제는 안됨
    