단순 선형회귀(Simple Linear Regression)
다중 선형회귀(Multiple Linear Regression)

통계 선형회귀
정규화 외 데이터 조작, 모델조작 허용X

머신러닝 선형회귀
예측을 잘해서 mse 줄인다면 조작해도 상관없음

선형회귀
    지도학습, 회귀, 평가지표 = mse
    학습 data에는 없는 새로운 data에 대한 값을 예측할 때
    데이터의 분포를 가장 잘 표현할 수 있는 직선을 그려서 값을 예측하는 방법
    y = wx + b
    변수가 많지 않기 때문에 과대적합을 통제하기 어려움
        -> 라쏘(Lasso), 릿지(Ridge)로 통제

mse = (측정값 - 예측값)^2

최소 mse 찾는 방법
    1. mse 미분

    2. 경사하강법(GD)
        mse가 최소가 되게 하는 최적의 w, b값을 찾는 방법론
        mse가 줄어들지 않을 때까지 미분을 반복하여 mse를 찾아냄
        학습률 조정 필요(작으면 섬세하지만 시간 오래 걸림, 크면 속도 증가하고 이동이 커짐)
        -> data 양이 많을수록, 특성의 개수가 많을수록 유리

        - 확률적 경사하강법(SGD)
            큰 data set에서 일반 경사하강법의 느린 단점을 보완하기 위한 방식
            전체 data가 아닌 일부 data만(표본집단)으로 w, b값을 업데이트
            -> 좋은 방향으로만 업데이트가 일어나지는 않음
            -> 수학적으로는 맞지 않지만 딥러닝에서 사용
            최적화 속도 빠름, 계산 비용 줄어듬

        - 미니 배치 확률적 경사하강법
            batch size를 나눠서 학습, 여러 개의 표본집단을 사용

규제(Regularization)
    선형 회귀모델에서 과대적합의 위험을 감소시키기 위해 w값의 비중을 줄이는 것(y축 조절)
    베타1가 커질수록 영향이 강해지면서 mse가 줄어드는데 의존도를 보정해주기위해 mse에 베타1를 더해줌
    -> 더해주면 베타1이 증가할수록 mse도 같이 증가하기 때문에 알고리즘이 mse를 줄일 다른 요인을 찾음
    mse + 베타1
    L1 규제 : Lasso -> 절대값
        절대값 베타1이 수렴시키는 것을 방해하기 때문에 알고리즘이 베타1을 지우려고 함
        특정항을 지울 때 사용
    L2 규제 : Ridge -> 제곱
        절대값 베타1 수렴 가능, 지우지는 않음

    규제 강도 : alpha = hyperparameter
    Lasso(alpha)
    Ridge(alpha)
    alpha가 커지면 규제효과가 커짐 -> 과대적합 감소, 오차 증가
        작아지면 규제효과가 작아짐 -> 과대적합 증가, 오차 감소
