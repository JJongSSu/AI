sklearn → GPU를 사용할 수 없음 → 대형 프로젝트에는 적합하지 않음

파이토치 : 딥러닝 프레임워크, 코딩+수학 실력자들이 사용

인공지능 → 통계적인 계산을 하기 때문에 명백한 답이 있어야 하면 사용하지 않음

애플, 구글 → 예전 아이폰이 스마트폰 시장 80%  점령 → 이 일을 계기로 삼성 안드로이드 폰이 역전 → 지금은 다시 애플이 역전...

predict - test 차이 = cost
기울기 조정하면서 weight를 찾아감
최적화 = weight를 찾아가는 과정, 적분값이 0이 되는 점을 찾는 과정

딥러닝
    학습할수록 결과가 좋아짐
    새로운 data를 계속 학습하기 때문

Learning Curve
    학습 data 양에 따라 성능 확인
    교차 검증하여 산출
    
    일자 : 과적합
    대각선 : 애매함
    위 곡선 : 좋음
    아래 곡선 : 안좋음

평가지표
Accuracy : (TP+TN) / (TP+TN+FP+FN) → 변하지 않음
        
        confusion matrix
            Accuracy 한계 보완하기 위해 만듬
            
            Precision(정밀도) : TP/(TP+FP) → 양성으로 예측한 것 중에서 진짜 양성인 확률
            Recall(재현도) : TP/(TP+FN) → 실제값이 양성인 것 중에서 진짜 양성인 확률
            Precision과 Recall는 반비례(FP-FN 관계)

            F1 accuracy_score 
                recall과 precision의 조화평균을 사용
                2*precision*recall / (precision + recall)
                → 현장에서 모델 사용 판단 기준으로 사용
            
            specificity(특이도) : 1 - (FP/(FP+TN))
            sensitivity(민감도) : TP/(TP+FN)
            → trade off 관계
            
            ROC curve : 호의 가장 볼록한 부분에서 타겟(Y축 꼭지점)까지의 거리를 통해 설명
            AUC : 호의 면적으로 설명, 최대값 1
            점선과 같은 직선으로 나오면 머신러닝 알고리즘으로 사용할 수 없음

이진분류(binary classification)은 다음과 같이 confusion matrix를 표현할 수 있다. 
                양성 예측               음성 예측
실제 양성   TP (True Positive)      FN (False Negative)
실제 음성   FP (False Positive)     TN (True Negative)
→ 네 가지 개념(TP, FP, TN, FN)를 기반으로 precision, recall, f1-score 등의 평가 점수를 만듬

Ai 사이트 : https://www.aihub.or.kr/

DNN
    논리값 → 비연속적인 값, classifier의 기본값
    선형 → 연속적인 값
    결정함수를 사용하면 0,1로 만들 수 있다 → 판단에 사용할 수 있음

    퍼셉트론
        신경망의 노드, 단위 로직

    단층 퍼셉트론(Perceptron)
        순서
            input - Weights(가중치) - Net input func(시그마) - Activation func(시그마 sum 값을 활성화 함수에 대입) - output
        input data 수 = weights 수
        input data 와 weights 1:1 적용
        임계치(활성화되기 위한 최소값)보다 크면 1, 작으면 -1 출력
        XOR 문제를 학습할 수 없다 -> 범위안에 있는 data는 학습할 수 없다
        
        ex) 전기전자 : AND, OR, NOT 캐드

    다층 퍼셉트론(MLP)
        순서
            입력 -> 은닉층(h1) -> 은닉층2(h2) -> 출력 : 순방향
        input data와 은닉층 m:n 연결
        은닉층이 많으면 학습은 잘되지만 시간오래 걸리고 비용 많이 들어감

        -> 역전파로 다층 퍼셉트론 한계 극복

    역전파        
        순방향으로 학습한 output(실제값)과 예측값의 오차를 계산해 오차값을 다시 처음으로 보내주어 보정하고 다시 학습시키는 방법
        학습할수록 오차가 줄어듬
        오차는 파라미터 선정과 모델에 따라 양수, 음수, 절대값 등 기준이 달라질 수 있음

        역전파 hyper parameter => 많음
            hidden_layer_size = 은닉층 개수
            activation = 활성화 함수 지정
                logistic(시그모이드)
                relu(default값)
                tanh
            alpha = L2규제(Ridge : 제곱값, mse에 더해주는 베타값 살림) 적용
            batch_size = data size
            max_iter = 학습 반복 횟수

        신경망 복잡도 추정
            매개변수 = 은닉층 개수와 은닉층 유닛 수
            조정방법
                1) 과대적합
                2) 규제 강화를 위해 alhpa값을 증가시켜 일반화 성능을 향상시킴

    sigmoid func
        어떤 값 이전에는 0(False), 이후에는 1(True)을 반환

    ReLU func
        임계값까지는 0(False), 임계값을 넘으면 자신의 값을 가짐

    ELU
        0을 계산하기 때문에 잘못하면 0이나 무한대에서 나오지 못함
        OR연산, 나누기 0과 비슷

    Drop out
        신경망의 일부를 사용하지 않고 학습하는 방법
        input은 들어가지만 hidden layer 부분에서 빠졌다고 생각
    

    