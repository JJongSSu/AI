<인공지능>
빅데이터 = volume(규모) + velocity(속도) + variety(다양성)
    속도 -> data는 수명이 있기 때문에 적시에 빠르게 수집하고 인사이트를 내야 됨

다크 data
    빅데이터 중 사용할 수 없는 data

structured data
    숫자, 행과 열로 이루어져 있음

unstructured data
    대부분 data 형식, 동영상, sns 사진, 자연어 data 
    -> 모든 data는 알고리즘에 넣기 위해 숫자로 치환해야 됨. 숫자만 보고 직관적으로 data가 무엇인지 알 수 없음

semi-structured data
    크롤링한 data(html 규칙에 따름)

전처리 = unstructured data -> structured data
    정규표현식 등을 통해 필요한 data를 뽑아내는 과정

인공지능
    연산능력 : 단순 계산, 제어(계산기, 세탁기 등)
    판단능력 : 조건에 따라 정해진 것 중 선택(진단, 퍼즐 등)
    추론능력 : data로부터 예측/추론(머신러닝)
    학습능력 : 학습을 통해 진화(딥러닝)
    창의력 : 새로운 것을 만듬(초 인공지능)

머신러닝 척도 -> 머신러닝으로 만들기 위해서는 4가지 척도로 변환 필요(딥러닝에서는 사용하지 않음)
    명목 : 임의로 나눈 척도(성별)
    서열 : 순서가 있음
    등간 : 덧셈, 뺄셈이 가능(온도, 시간)
    비율 : 곱셈, 나눗셈 가능(무게, 길이)

딥러닝
    GAN
        이미지를 숫자로 1차 변환, 숫자를 다시 이미지로 변환 -> 처음에는 잘 안맞다가 맞을 때가지 여러번 돌림 
        -> 그림을 가장 잘 설명하는 숫자를 찾아냄 

    알파고 : 강화학습 -> 시뮬레이션
        ** 인공지능 알고리즘을 만들기 위해서는 변수가 어느정도 예측 가능해야 하고 보상이 명확해야 함
        게임 같은 경우 보상이 애매하기 때문에(도중에 나가면 피드백이 어렵기 때문에)
        스타가 안되면 롤은 가능?? 5개 챔프 다 조종 가능??(스킬 다 피하는 핵이 지금 있는데)

과적합
    overfitting(과대적합)
        train data에 너무 과도하게 학습되어 train data에만 잘 동작하고 test에서는 예측성능이 저하되는 현상
        Bias 증가

    underfitting(과소적합)
        train data를 충분히 반영하지 못해 train, test data 모두에서 예측성능이 저하되는 현상(학습을 제대로 하지 못한 것)
        Variance 증가

    Bias = 평균으로부터 멀어진 정도 = 예측값 - 측정값
        -> 예측값에서 멀리 떨어져 있으면 bias 높다고 판단
    Variance = 각 data간 떨어진 정도 = 측정값의 평균 - 측정값
        -> 측정값들이 퍼져 있으면 vatiance 높다고 판단
    => 머신러닝에서는 둘 다 줄여주는 것이 좋음

    Bias-Variance trade-off
    Bias는 학습량(동일 data)이 많아질수록 올라감
    Variance 학습량(동일 data)이 많아질수록 내려감
    두 개의 그래프가 만나는 점에서 학습을 종료시킴 => 일반화

    mse =  (측정값-예측값)^2
    mse 제곱하는 이유
        1) -값 보정
        2) 큰차이(분산)에 패널티 주기 위해 -> Bias, Variance를 고려하기 위해, 편향된 data를 만들지 않기 위해
        3) 전구간 미분 가능하게 하기 위해
            절대값을 사용하면 미분할 수 없음

    해결방법
    Generalization(일반화)
        모델이 처음보는 data에 대해 정확하게 예측할 수 있는 상태

    많은 다양한 data를 가지고 학습시킴

    data가 추가되면 test / train set에 넣을지 뭘보고 결정??
    고객 요구는 평가를 해주는거고 추가 data 입력은 7:3으로 나눠 train 시키고 test함. 기존 test set은 train에 추가해서 학습시켜도 됨   

<머신러닝>
지도학습
    레이블이 있음 -> 의도한대로 학습시킬 수 있음, input을 주었을 때 할당된 레이블을 맞추도록 하는 방법

비지도학습
    레이블이 없음, 컴퓨터 스스로 유사정도를 기반으로 묶음을 나눔 -> 결과를 보고 판단해야 됨

강화학습 
    레이블=보상

오토인코더
    레이블=input data

머신러닝 순서
    data -> knowledge -> information -> insight -> wisdom -> understanding

머신러닝 종류
    - 지도학습 : 분류, 회귀
        categorical 분류 : 레이블 범주형, 셀 수 있는 경우의 수, 정확하게 결과를 맞춰야 함(true, false), 절대평가
        continuous 회귀 : 레이블 연속형, 경우의 수가 무수히 많을 때, 실측값과 예측값의 차이로 평가, 상대평가, 
                        분류보다 결과가 정확한 값이 나오기 때문에 예외성 판단에 좋음
        -> 향수 추천
    
    - 비지도학습 : 기하, 확률
        기하 : 좌표평면에 나타냈을 때 비슷한 data끼리 묶는 방법
        확률 : 
            1. data들 간의 거리를 측정해서 가장 긴 거리를 기준으로 data를 분류
                n차원 -> n-1 차원의 기준을 가짐
                단위 설정이 중요, 한 data가 너무 튀면 자르는 기준이 그 축 중심으로 달라질 수 있음
            2. 등장 단어 빈도수로 분류
        -> 뉴스, 고객관리(성향)
        -> 클러스터링 : 비슷한 특성끼리 묶는 수학적 방법 -> 특정 고객층 공략 가능

    - 강화학습
        보상과 벌칙을 통해 스스로 방법을 찾도록하는 방법

    - 오토인코더
        GAN 알고리즘과 비슷한 학습 방법
        파일 보안 -> 회사 내에서 usb로 파일 받아가서 집에서 열어보면 형식이 갈려있음.
        옛날 영화에 PPL 넣고 재방영할 때 오토인코더 사용해서 숫자로 변환 후 PPL 제품을 넣어주면 영화 재생할 때 PPL 물건이 들어가있음 

학습과정
    : 문제정의 - data 수집 - 전처리 - EDA - 모델 선택 - 모델 학습 - 모델 평가
    
    1. 문제정의
        비지니스 목적 정의
            해결해야하는 문제가 어떤 것인가, 해결하여 얻고자 하는 것이 무엇인가

    2. data 수집
        크롤링, csv, 센서, 통계청, 설문조사

    3. 전처리
        결측치 및 이상치 처리, 불필요 data 제거
        정규표현식, data 가공
        특성공학(feature engineering)
            1) 특성 선택(feature selection)
            2) 특성 결합(feature combination)
            3) 특성 추출(feature extraction)
                다중공선성 해결하기 위해 사용, 직관성이 떨어짐, 두 개의 분산을 합친 하나의 분산을 만듬
            4) scaling : 단위변환하여 단위 통일 
            5) encoding : 문자형 data -> 수치형 data
            6) binning : 수치형 data -> 문자형 data, 특정 구간을 나누는 방법
                다시 숫자로 encoding하고 싶을 때 명목척도인지 서열척도인지 헷갈릴 수 있기 때문에 0과 1로 나타냄(설문조사)
                young 경활일 old
                1     0     0
                0     1     0
            7) normalization(정규분포화) : (x-평균)/표준편차 = (x-평균)/(분산)^2
                min-max scaling : x-min/max-min (0~1 사이값)

    4. EDA(탐색적 data 분석)
        시각화
        histogram(빈도수), boxplot(평균, 중간값, 사분위수(0~4분위 : 중위(2분위)), 이상치 등)
        scatter plot(산점도), plot(선)

    5. 모델 선택
        목적에 맞는 모델 선택
        hyper parameter

    6. 모델 학습
        train-test spilt
        - model.fit(X_train(문제), y_train(답))
            train data와 test data를 7:3으로 분리
            train data로 학습
        
        - model.prdict(X_test)
            test data를 넣고 정답 예측

    7. 모델 평가 분류
        pre = model.predict(X_test)
        metrics.accuracy_score(pre,y_test)
            test data를 넣어 예측한 정답(pre)과 실제 정답(y_test)를 비교 -> 0.94
        분류 : 얼마나 많이 맞았는지
        회귀 : 실제값과 예측값의 차이의 제곱을 확인
            mes(x-m)제곱하는 이유
                1) -값 보정
                2) 큰차이(분산)에 패널티 주기 위해
                3) 전구간 미분 가능하게 하기 위해
                    절대값을 사용하면 미분할 수 없음
        
        mes는 모델에 따라 달라질 수 있음
        정확도 -> 직관적

        Accuracy(오차행렬) 한계
            AI가 잔머리 써서 불균형 data를 한쪽으로 밀어버릴 때
            틀려도 잘 틀려야 될 때
                양성 -> 음성
                음성 -> 양성
        
Accuracy : (TP+TN) / (TP+TN+FP+FN) -> 변하지 않음

Precision(정밀도)
    TP/(TP+FP) → 양성으로 예측한 것 중에서 진짜 양성인 확률

Recall(재현도)
    TP/(TP+FN) → 실제값이 양성인 것 중에서 진짜 양성인 확률
    Precision과 Recall는 반비례(FP-FN 관계)

F1 accuracy_score 
    recall과 precision의 조화평균을 사용
    2*precision*recall / (precision + recall)
    → 현장에서 모델 사용 판단 기준으로 사용
    
    specificity(특이도) : 1 - (FP/(FP+TN))
    sensitivity(민감도) : TP/(TP+FN)
    → trade off 관계
    
    ROC curve
        호의 가장 볼록한 부분에서 타겟(Y축 꼭지점)까지의 거리를 통해 설명
    AUC
        호의 면적으로 설명, 최대값 1
        점선과 같은 직선으로 나오면 머신러닝 알고리즘으로 사용할 수 없음

비지도학습
    구조를 가정하고 data를 구조에 맞추어 설명하는 방법
    정답 값이 주어지지 않은 입력 data들을 분석하여 공통점이 있는 data끼리 묶어 그룹핑하는 방식
        -> x_train만 제공
    
    1) 종류
        (1) 차원축소 : PCA, t-SNE, NMF
            X-1 차원으로 재축소
        
        (2) 군집 : K-Means, Hierarchial, DBSCAN
    
    2) 장단점
        학습결과를 평가하기 어렵다 -> 묶인 군집 data들을 확인해야함
        EDA, 전처리에서 활용하기도 함
        scale 조정 메서드도 지도 정보(supervised information)을 사용하지 않으므로 비지도 방식
            scaler -> x_train data만 입력해줌

    1. 차원축소
        1) PCA
            text -> 잠재의미분석
            이미지 -> 고유이미지

            순서
                (1) 분산이 가장 큰 방향(벡터)를 찾음
                (2) 찾은 벡터와 직교하면서 가장 큰 정보를 담은 벡터를 찾음(n차원에서 n개)
                (3) 주성분을 구한 후 기존 data에서 평균을 구함 -> 기존 data로 돌려보냄

            분산에 대한 정보 유지, 정보 손실 적음, 선형 data에 적합

            pca.fit(x_train)
            pca.transform(x_test)
            -> x_train에 대해 학습시켜도 x_test data를 변환할 수 있음

        2) t-SNE
            데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾는 것 -> 3개 이상 변수를 가지지 않음
            x data를 넣어주면 x에 대해서만 작동
            다른 data를 넣어주려면 다시 fit과 transform을 해줘야함
            가까운 포인트를 유지하는게 더 큰 목적 -> 이후 포인트에 대한 보정    
            비선형 data에 적합, 시각화에만 도움

        3) 장점
            다중공선성 해결 가능 -> feature 간 독립
            많은 정보를 효과적으로 시각화해서 data 특성 탐색 가능
            연산 속도 향상

        4) 단점
            정보 손실 발생
            결과 data 해석하는데 어려움이 있음


    2. 군집분석
        데이터를 비슷한 것끼리 일정한 군집으로 나누는 것 -> 클러스터링(cluster)

        1) 하는 이유
            비슷한 군집에 대해 대응할 수 있다
            데이터가 많이 있을 때 군집분석하면 data를 하나씩 살펴보지 않아도 된다

        2) 종류
            1. 계층적 군집분석(hierarchical clustering) : 거리기반
                반복해서 data를 뭉치거나 쪼개는 방식으로 군집을 만드는 방법

                분석순서
                    1) 하나의 data를 하나의 군집으로 봄
                    2) 모든 군집들 사이의 거리를 잰다
                    3) 거리가 가장 가까운 두 군집을 하나의 군집으로 합친다
                    4) 원하는 갯수의 군집이 될 때까지 반복
                
                거리재는 방법 : method = 5가지
                    single : 가장 가까운 data들의 거리 
                    complete : 최대 거리가 가장 짧은 data의 거리 
                    average : 모든 data 사이의 거리 평균 
                    centroids : 군집 중심과 중심 거리
                    ward : 두 군집을 합쳤을 때 군집 내 거리 분산의 변화를 거리의 척도로 삼음
                        _DCX 'ward'

            2. k-means : 거리기반
                data와 군집 중심점의 거리를 계산해서 가장 가까운 군집에 속한다고 가정
                
                분석순서
                    1) k개의 중심점을 무작위로 선정
                    2) 중심점과 거리를 재서 가장 가까운 군집에 data를 할당
                    3) 군집에 속한 멤버들의 평균을 내서 중심점을 다시 정한다
                    4) 중심점에 변화가 없을 때까지 반복

            3. DBSCAN : 밀도기반
                최초의 임의의 점 하나로부터 퍼져나가는 밀도에 기반한 군집화 방식
                군집의 개수를 정하지 않음
                
                Epsilon : 군집 반경
                minPts : 군집 이루는 최소 data 수
                core point : 반경 Epsilon 내에 minPts개 이상 점이 존재하는 중심점
                border point : 군집에 속하는 점
                Noise point : 어떤 군집에도 속하지 못하는 점

교차검증(cross-validation)
    학습-평가 data를 골고루 설정하여 모델의 안정성을 높이고 과대적합을 감소시키는 통계적 기법
    data1(x_train, y_train)
    data1(x_test, y_test)
    -> test set에 맞게 학습 될 수 있다
    -> 학습 후 검증

    holdout-cross-validation
        data를 학습, 검증, 평가 data로 나눔 -> 2번 검증
        학습    검증    평가

    k-fold cross-validation : 회귀
        data를 k개로 나눈 후 1번째 세트를 제외하고 학습 -> 1번째 세트를 이용해서 평가(accuracy) -> 마지막 세트까지 반복 -> 각 세트에 대해 평가 
        -> 각 평과 결과의 평균을 구함
        hyper parameter cv = data를 몇 개로 나눌지 결정
        여러 개의 data set을 한가지 모델을 이용해서 평가하는 방식

    장점
        모든 data set을 학습과 평가에 활용하기 때문에 안정적이고 정확 -> 통계적 기법으로 과대적합 감소 -> 일반화 도움
        모델이 훈련 data의 변경에 대해 얼마나 민감한지 파악 가능
        data set 크기가 크지 않아도 유용하게 사용 가능

    단점    
        여러 번 학습하고 평가하는 과정을 거치기 때문에 계산량 많아짐 -> 시간, 비용 증가

텍스트 마이닝
    자연어 처리방식, 문서처리 방법
    자연어 : 모든 언어 <-> 인공언어 : 특정 목적을 위해 인위적, 의도적으로 만든 언어

    인간의  언어가 사용되는 실세계 모든영역
        - 정보검색, 질의응답
            구글, 네이버, siri, bixby, IBM Watson
        - 기계번역, 자동통역
            구글번역, 네이버 파파고, ETRI 지니톡
        - 문서작성, 문서처리 등
    
    활용사례
        1. 지식경영 : 의미있는 data만 뽑아냄
        2. 사이버 범죄 예방 : 특정 단어 분류 -> 범죄 예측, 스팸 분류
        3. 고객 관리 서비스 : 자동화된 응답 제공 -> 챗봇
        4. 콘텐츠 요약 : 회의록 작성
        5. 소셜 미디어 데이터 분석 : 다양한 의견과 감성반응 살핌 -> 리뷰 data 크롤링
    
    텍스트 분류, 감성분석, 텍스트 요약, 군집화 및 유사도 분석

    말뭉치 > 문서 > 문단 > 문장 > 단어 > 형태소
        하위 단계의 집합

    분석 프로세스
        텍스트 수집 -> 전처리 -> 토큰화 -> 특징 추출(임베딩) -> data 분석
            - 텍스트 수집 : 웹 크롤링, 빅카인즈 뉴스 data, NDSL 논문 data
            - 전처리 : 오탈자 제거, 띄어쓰기, 불용어(의미없는 단어) 제거, 정제(노이즈 data : 빈도가 적은 단어, 의미없는 특수문자), 
                    정규화(비슷한 의미 단어 통합), 어간 추출(단어 중 핵심부분만 추출), 표제어 추출(유사한 단어들에서 대표 단어 추출)
            - 토큰화 : 말뭉치를 나누는 작업(공백(영어 text), 형태소, 명사, 단어기준), 기준은 분석 방법에 따라 다름
                    감성 분석 -> 한글에서 감성을 나타내는 품사가 동사, 형용사에 가깝기 때문에 한글 형태소 분석기를 사용해서 동사, 형용사 위주로 추출
            - 특징 추출 : 임베딩(text data를 벡터형태로 변환), 중요한 단어 선별(적은 수의 문서에 분포, 분서 내에 빈도 많아야 함)
                        -> 모든 문서에 분포되어 있으면 차별성이 없음
            - 분석 : 머신러닝, 딥러닝
    
        토큰화 종류
            단어(word) 단위, 문자(character) 단위 
            n-gram 단위 : 여러 개의 단어를 하나의 단어로 치환, 벡터를 1개로 만들어 줌
        
        임베딩
            - 원핫인코딩 : 포함되어 있으면 1, 아니면 0
                토큰에 고유번호를 배정 -> 모든 고유번호 위치의 한 칼럼만 1, 나머지 컬럼은 0인 벡터로 표시하는 방법

            - BOW(bag of word) : CounterVectorize, TF-IDF
                - CounterVectorize : 빈도수를 기반으로 벡터화(결과값 = 각 벡터의 빈도수) 
                    -> 단어의 순서나 중요도를 고려하지 않기 때문에 문맥의 의미를 반영하기 힘듬
                - TF-IDF : 개별 문서에서 자주 등장하는 단어에 높은 가중치 부여, 모든 문서에 자주 등장하는 단어는 패널티 부여(단어의 중요도 반영)
                    TF = 단어가 각 문서에서 발생한 빈도
                    DF = 단어가 등장한 문서의 수
                    IDF = DF의 역수
                    적은 문서에서 많이 발견될수록 가치 있는 정보
                    희소성을 반영하기 위해 TF*IDF 값 사용

                    W = TF * log(N/DF)
                        N = 전체 문서의 수
                        N이 커지면 IDF가 기하급수적으로 커지는 것을 방지하기 위해 log 사용

            - Word2Vec
                단어 벡터 사이의 거리 측정
                - CBOW
                    맥락으로부터 타겟을 추측하는 신경망

                - Skip gram
                    타겟으로부터 주변 맥락을 추측하는 신경망
                    역전파 사용 -> 여러 단어의 가중치를 알 수 있음

data scaling(데이터 스케일링)
    데이터 특성들의 값 범위를 일정한 수준으로 맞춰주는 작업

    특성마다 다른 범위를 가지면서 편차가 큰 data의 경우 모델들이 잘못된 결과를 도출할 가능성이 있음
    -> DT(결정트리) 제외 모든 모델 scaling 필요

    거리, 수치 기반 모델 적용시 특성들을 비교 분석하기 쉽게 만들어 예측에 도움을 줌
    회귀 모델에서 학습 안정성과 속도 개선
    트리기반 모델 등 거리 값에 관계없는 모델들은 scaling 해줄 필요 없음
    데이터 분포가 고르다면 반드시 해야 하는 것은 아님

    주의점 : train, test data에 같은 변환을 적용해야 함(예측값의 범위가 달라질 수 있음)
            학습 data의 데이터 분포를 가지고 학습한 모델로 테스트 data도 변환 해줘야 함
            scaler.fit.transform(x_train)
            scaler.transform(x_test)

    feature마다 다른 범위를 가지는데 편차가 클 경우 결과가 잘못 도출될 수 있음
    트리 기반 모델 제외 모두 scaling 필요

    1. standard : 정규분포 형태로 변환, 이상치에 민감 -> 이상치가 평균과 분산에 영향을 미침
    2. minmax : 0~1 사이 값으로 변환(-값이면 -1~1) -> 이상치가 있으면 사용하기 힘듬
    3. normalizer : 지름 1인 원에 투영, 방향(각도)만 중요할 때 사용(단어의 유사도 판단)
    4. robust : 25~75% data만 사용, 이상치에 민감 X, (x-q2)/(q3-q2) : q2 = 50%, q3 = 75%

    train data와 test data에 같은 변환을 적용해야 함 -> 예측값의 범위가 달라질 수 있음
    data 나누기 전에 scaling 먼저 진행하고 train, test data로 나눠도 됨

    종류
        standard scaler
            평균 0, 분산 1인 정규분포 형태로 변환
            이상치가 있다면 평균과 분산에 영향을 미쳐 변환된 data 분포는 달라짐

        minmax scaler
            0~1 사이의 값으로 변환(-값이 있다면 -1~1사이로 변환)
            이상치가 있다면 사용하기 힘듬

        normalizer
            지름 1인 원에 투영
            거리는 상관없고 방향(각도)만 중요할 때 사용
            ex) 단어의 유사도 판단

선형회귀
    단순 선형회귀(Simple Linear Regression)
    다중 선형회귀(Multiple Linear Regression)

    통계 선형회귀
    정규화 외 데이터 조작, 모델조작 허용X

    머신러닝 선형회귀
    예측을 잘해서 mse 줄인다면 조작해도 상관없음

    선형회귀
        지도학습, 회귀, 평가지표 = mse
        학습 data에는 없는 새로운 data에 대한 값을 예측할 때
        데이터의 분포를 가장 잘 표현할 수 있는 직선을 그려서 값을 예측하는 방법
        y = wx + b
        변수가 많지 않기 때문에 과대적합을 통제하기 어려움
            -> 라쏘(Lasso), 릿지(Ridge)로 통제

    mse = (측정값 - 예측값)^2

    최소 mse 찾는 방법
        1. mse 미분

        2. 경사하강법(GD)
            mse가 최소가 되게 하는 최적의 w, b값을 찾는 방법론
            mse가 줄어들지 않을 때까지 미분을 반복하여 mse를 찾아냄
            학습률 조정 필요(작으면 섬세하지만 시간 오래 걸림, 크면 속도 증가하고 이동이 커짐)
            -> data 양이 많을수록, 특성의 개수가 많을수록 유리

            - 확률적 경사하강법(SGD)
                큰 data set에서 일반 경사하강법의 느린 단점을 보완하기 위한 방식
                전체 data가 아닌 일부 data만(표본집단)으로 w, b값을 업데이트
                -> 좋은 방향으로만 업데이트가 일어나지는 않음
                -> 수학적으로는 맞지 않지만 딥러닝에서 사용
                최적화 속도 빠름, 계산 비용 줄어듬

            - 미니 배치 확률적 경사하강법
                batch size를 나눠서 학습, 여러 개의 표본집단을 사용

    규제(Regularization)
        선형 회귀모델에서 과대적합의 위험을 감소시키기 위해 w값의 비중을 줄이는 것(y축 조절)
        베타1가 커질수록 영향이 강해지면서 mse가 줄어드는데 의존도를 보정해주기위해 mse에 베타1를 더해줌
        -> 더해주면 베타1이 증가할수록 mse도 같이 증가하기 때문에 알고리즘이 mse를 줄일 다른 요인을 찾음
        mse + 베타1
        L1 규제 : Lasso -> 절대값
            절대값 베타1이 수렴시키는 것을 방해하기 때문에 알고리즘이 베타1을 지우려고 함
            특정항을 지울 때 사용
        L2 규제 : Ridge -> 제곱
            절대값 베타1 수렴 가능, 지우지는 않음

        규제 강도 : alpha = hyperparameter
        Lasso(alpha)
        Ridge(alpha)
        alpha가 커지면 규제효과가 커짐 -> 과대적합 감소, 오차 증가
            작아지면 규제효과가 작아짐 -> 과대적합 증가, 오차 감소

KNN(K-Nearest Neighbors)
    기하 모델
    주변 data를 보고 새로 들어온 data를 판단
    최근접 data를 기준으로 판단하는 알고리즘
    KNN(k=5)
    k = 홀수로 지정(변수 개수의 배수는 피하기)
    k값이 너무 적으면 과대적합, 너무 많으면 과소적합이 일어남

    Hyperparameter(주요 매개변수) : 사람이 정하는 값
    parameter : 컴퓨터가 정해준 값(그래프 기울기, y절편 등)

    **변수 값 범위 재조정
        차원의 단위(x축, y축)를 조정
        단위를 비교할 수 없는 경우 단위 통일 필요 -> 편차를 통해 통일시킴
            - Min-max normalization
                x-min/max-min (0~1 사이값)
            - Z-score standardization
                (x-평균)/표준편차 = (x-평균)/(분산)^2
        -> 방법에 따라 결과가 달라질 수 있음

    장단점
        새로운 관측치와 각각의 학습 data 사이의 거리를 전부 측정해야 하기 때문에 계산이 오래걸림
            학습시간은 별로 안걸리지만 예측 시간이 오래 걸림 -> 새로운 data가 들어왔을 때 기존 data와의 거리 계산을 시작하기 때문
            -> KNN => lazy algorithm
            -> 실시간 예측해야할 때는 사용하지 않음
            -> 일반 머신러닝은 학습이 오래걸리고 예측시간이 별로 안걸림
        훈련 data set 크기(특성 수, data 수)가 크면 예측이 느려짐
        기하적 모델이기 때문에 data scale 조정 필요
        이해하기 쉽고 조정없이도 성능이 나쁘지 않기 때문에 가볍게 직관적으로 볼 때 사용

    1. 유클리디안 거리(Euclidean Distance)
        피타고라스 정리와 동일
        3차원 이상에서도 정의 가능(피타고라스는 2차원에서만 적용가능)
        math.sqrd((x1-x2)^2 + (y1-y2)^2)
        최단거리

    2. 맨해튼 거리(Manhattan Distance)
        계산속도 빠름(계산 비용 줄일 수 있음), 목표지점까지 가기만 하면 된다 -> 계산 단순화
        |x1-x2| + |y1-y2|

로지스틱 회귀(Logistic Regression)>
    분류, 기하 모델 -> 단위 중요
    회귀와 분류를 동시에 사용하기 위해 로지스틱 회귀 사용
        (회귀모델을 짠다음 로스스틱 회귀를 통해 분류하도록 하는 방법)

    선형모델 방식을 분류에 사용
        선형모델은 간단한 함수식을 사용하기 때문에 학습 및 예측 속도가 빠름
        큰 data set에도 잘 동작
        일반적으로 특성이 많을수록 더 잘 동작

    선형회귀 직선을 통해 두 집단을 나눌 수 있다

    Hyper parameter
        C : 규제강도의 역수
        max_iter : 경사하강법을 위해 반복 횟수   머신러닝 Iteration <-> 딥러닝 Epoch
        L2규제 사용, 중요한 특성이 별로 없다면 L1 사용해도 됨

    Sigmoid 함수 사용 
        -> 모든 분류 값을 0~1 사이의 확률 정보로 표시 가능
        y = 1 / (1 + e^(-x))
        0.5 기준으로 낮으면 0, 높으면 1로 예측 -> 몇 %로 0인지, 1인지 판단 가능
            x=0, 0.5 : 존재하지 않음
            x=무한대, 1 : 1번 레이블
            x=-무한대, 0 : 0번 레이블
        -> 어떤 회귀선이든 0~1사이 값을 가짐, 중심은 50%로 수렴
        -> 회귀 모델을 분류모델로 만들 수 있음

        기하적 모델 : 좌표평면에 찍을 수 있다, 단위에 따라 나뉨
        확률 모델 : 빈도를 알 수 있다, 동시출현에 따라 나뉨
        -> 기하적 확률 모델
            표본집단 면적 / 모집단 면적
            선일 때는 0 -> 면적과 선의 개념이 다르기 때문에, 면은 선의 무한한 모임 -> (선/모집단면적)에서 모집단 면적이 무한대이기 때문에 0으로 수렴
            x<0, 0<y<0.5 = 0번 레이블
            x>0, 0.5<y<1 = 1번 레이블
            x=1, y=0.5 : 나올 확률 0이기 때문에 고려하지 않음 -> (점/선)에서 선은 점의 무한한 모임이기 때문에 0으로 수렴
            -> 따라서 2진법 외에 사용할 수 없다

    단점 : 2진 분류, 레이블이 2개일 때만 사용 가능
        2개를 넘어가면 레이블의 범위가 달라지기 때문에 공평하지 않음
        양쪽 끝은 무한대로 수렴하기 때문
        선형을 전제 만족

    보완방법(다진분류)
        <lib 사용 가능>
        - Softmax
            (ei)^x / ((e1)^x+(e2)^x+(e3)^x+...+(ek)^x)
            e = 레이블의 개수
            i = 해당 레이블
            해당 레이블에 할당될 확률을 구하는 방법, 0~1 사이값
        
        <직접코딩>
        - OVO
            이진분류 model을 여러 개 만들어 리그형식으로 매치시키는 방법
            중복이 되면 많이 나온 결과값으로 결정(A,A,B -> A)
                model 1 = A vs B
                model 2 = B vs C
                model 1 = A vs C
            Softmax보다 성능이 좋음
            분류모델은 레이블 개수가 많아질수록 성능이 떨어짐
            여러 개의 model을 돌려야 되기 때문에 시간이 오래걸림

        - OVR
            여집합 사용
            먼저 model 1을 돌려 A를 뽑아내고 나머지 여집합 중에서 다시 model 2를 돌려 B를 뽑아내는 방식
                model 1 = A vs A^c
                model 2 = B vs B^c
            OVO보다 계산 시간 빠름, 결과가 겹치지 않음
            data 불균형이 발생함 -> model이 한쪽으로 밀어버릴 수 있음
                (A B C가 각각 30 30 30이면 30:60이 되기 때문)

손실함수 : 최적화 척도 판별하는 기준

Decision Tree(결정 트리 모델)
    확률적 모델

    알고리즘
        Tree를 만들기 위해 Y/N 질문을 반복하며 학습
        노드 + 엣지
        중요 노드를 위에 설정

    지니 불순도(gini impurity) = 손실함수
        모델이 제대로 fitting 되었는지 평가하는 기준
        최적의 hyper parameter인지 판단
        낮을수록 최적화
        Entropy(불확실성)

    hyper parameter
        max_depth : 트리 최대 깊이, 값이 클수록 모델의 복잡도가 올라감
        max_leaf_nodes : 리프 노드의 최대 개수
        min_sample_leaf : 리프 노드를 구성하는 최소 샘플 개수

        깊이가 정해지면 리프 노드 개수는 정해지는거 아님??
        리프 노드가 무조건 O/X로 결정되는 것이 아니기 때문에 개수를 정해주어야 함

    장점
        확률모델이기 때문에 정규화 필요 X -> scaling을 하지 않기 때문에 전처리 거의 필요 없음
        확률모델 -> %로 구할 수 있기 때문에 회귀모델로 활용 가능
        직관적 -> 이해 쉬움
        hyper parameter가 많기 때문에 조절할 수 있는 변수가 많다
        트리 구성 시 특성의 중요도를 계산하기 때문에 특성 선택에 활용할 수 있음
        계량 기법들이 많고 성능이 좋음(gradient boosting...) : kaggle 단위(개인 컴퓨터 내에서 돌릴 수 있는 범위)

    단점
        트리 깊이가 깊고 복잡해지면 과대적합 위험성 매우 높음
        트리 최대 깊이, 리프 노드 최대 개수를 지정(작을수록 복잡도 내려감)
        리프 노드를 구성하는 최소 data 개수를 지정(클수록 복잡도 내려감)
        노드를 잘못 탔을 때는 보정할 수가 없기 때문에 과대적합 심함

    과대적합 제어(가지치기)
        drop out 방식
        적절한 수준에서 트리의 nodes 일부를 자르거나 합쳐 줌으로써 제어


    랜덤 포레스트(Random forest)
        여러 개의 결정 트리 모델로 예측한 값을 투표 -> 최종 선택
        hyper parameter, 노드 배치 조절하여 모두 다른 결정 트리로 구성

        장점 : 실제값에 대한 추정값 오차 평균화, 분산 감소, 과적합 감소

앙상블
    여러 개의 모델이 예측한 값을 결합하여 정확한 예측을 하는 기법

    사용하는 이유
        1. 단일 모델에 비해 높은 성능과 신뢰성을 얻을 수 있음

        2. 여러 모델을 사용하기 때문에 data가 적은 것에 대해 학습 효과를 얻을 수 있음
            과대적합 방지

    <종류>
        보팅 : 여러 개의 다른 종류의 모델로 결과를 예측하여 투표/평균을 통해 최종 선정
            하드 보팅 : 투표 -> 다수결
            소프트 보팅 : 각 확률의 평균

        배깅 : 같은 모델을 가지고 조건을 다르게 여러 개 만들어서 각 결과값을 종합하여 투표/평균을 통해 최종 결과를 내는 것
            각 모델 간 독립적으로 병렬처리, 동시에 진행
            data 값들의 편차가 클 경우 사용, 과대적합 방지
            data 무작위 선택
            랜덤 포레스트 : 여러 개의 결정 트리 모델로 예측한 값을 투표를 통해 최종선택하는 모델
                다수결, 오차 평균화
                과대적합을 통계적 방법으로 해소
                부스팅에 비해 수행속도 빠름
                모델 튜닝 시간 많이 필요 -> 변수 선정 가능-> feature importance 확인 가능
                큰 data 세트에도 잘 동작하지만 트리 개수가 많아질수록 시간이 오래 걸림
                
                단점 : 선형모델보다 예측 느림

                결정트리
                    직관적 -> 결과 이해하기 쉬움 
                    과대적합 되기 쉬움
                결정트리의 과대적합 보완하기 위해 랜덤 포레스트 사용

        부스팅 : 순차적 학습 모델
            잘못 예측한 data들에게 가중치를 주어 업데이트 -> 다음 모델로 넘어감 -> 다시 분류 -> 다시 data 업데이트 -> 다음 모델
            학습 정확도가 낮거나 오차가 클 경우 사용, 과소적합 방지
            data 무작위 선택(가중치 적용)
            Ada, Gradient, XG, Light GBM
            XG : early stopping(더 이상 학습효과가 없을 때 멈추기 위한 hyper parameter) 제공, 과대적합 방지


                랜덤포레스트                                부스팅 -> 트리 계열
                분류/회귀                                   분류/회귀
                Scaling 필요 없음                           Scaling 필요 없음
                비선형                                      비선형
                hyper parameter 많음                        hyper parameter 많음
    장점        높은 범용성                                 높은 범용성
    단점        단일 모델에 비해 시간 오래걸림               시간 오래 걸림, 과대적합

SVM
    신경망 알고리즘보다 사용이 간결
    분류나 회귀 분석에 사용 가능하지만 분류에서 주로 사용
    *다른 레이블(클래스) 사이에 존재하는 여백(거리=margin)을 최대화하려는 목적으로 설계
    support vector와 Hiperplane을 이용해서 분류를 수행하는 알고리즘
    선형으로 분리할 수 없는 점들을 분류하기 위해 커널 사용
    커널은 더 높은 차원의 data로 변환
    RBF 커널 hyper parameter = gamma -> 너무 크면 과대적합 발생
    
    장점 : 
        오류 data에 대한 영향이 적음 -> C값(이상치 허용 정도)
        과대적합 적음
        신경망보다 사용하기 쉬움
    
    단점 : 
        파라미터 조합 성능 test가 필요함
        학습 속도느림
        블랙박스 형태 -> 모델 해석이 어렵고 복잡

    Hiperplane : N차원 공간에서 N-1차원으로 나눠주는 subspace
    support vector : data set 안에 포함되어 있는 두 가지 레이블로 구분되는 data 중 최외각에 있는 샘플들의 vector
                    Hiperplane과 가장 가까운 vector
    margin : support vector통해 구한 두 레이블 사이의 거리 -> 최대화 하는것이 목적
            Hiperplane과 support vector 사이의 거리

    Hiperplane을 잘 분류하는 기준
        한쪽에 치우치지 않도록 분류
        양쪽 data와 균등한 위치에 분류 기준 세우기
        -> 각 레이블에 대해 margin 길이가 동일하게 설정

    SVM 분류 방법
        1. 두 레이블 중 어느 하나에 속한 data 집합이 주어짐
        2. 주어진 data set을 바탕으로 새로운 data가 어느 레이블에 속하는지 판단하는 비확률적 선형분류모델을 만듬
        3. 만들어진 분류 모델은 data가 있는 공간에서 경계로 표현 -> 가장 큰 폭을 가진 경계를 찾음

    이상치 허용 (hyper parameter = C : 높을수록 이상치 허용 안함)
        하드 margin : 이상치 허용 거의 안함, C값 높음
            경계선이 곡선형태가 됨, C값 너무 커지면 과대적합
        소프트 margin : 어느정도 이상치 허용, C값 낮음 -> 모델이 과대적합 되지 않고 어느정도 일반화된 모델을 만들겠다
            C값 너무 작아지면 과소적합

    kernal SVM
        1. 비선형
        2. polynomial(다항식)
            N+1차원으로 꺼내서 결정경계(Hiperplane)를 긋는 방식
        3. RBF(Radial Bias Function) : 가우시안
            무한한 차원으로 변환 -> 선형대수 사용 -> 다차원의 결정경계 긋는 방식
            hyper parameter = gamma 
                : 각 data들의 영향 범위를 조정, 값이 커지면 하나의 data가 큰 영향범위를 가짐 -> 결정경계가 곡선을 이룸, 값이 작아지면 결정경계 일반화
                    gamma 너무 크면 과대적합, 너무 작으면 과소적합

            굳이 다차원으로 안만들어도 되는 경우에도 그냥 가우시안 사용??

        hyper parameter
            C(이상치 허용)
                허용 O(소프트 margin) : margin 커짐 -> 너무 허용은 안되지만 일반적으로 허용하는 방향으로 만들어주기
                허용 X(하드 margin) : margin 작아짐

            gamma(data의 영향범위) : 결정경계 곡률 결정
                값이 커지면 과대적합, 작아지면 과소적합

머신러닝 순서
    1. data set -> 학습 data, test data 나눔 -> dataframe화
    2. scaling -> scaling한 data를 다시 dataframe화
    3. EDA -> 어떤 feature를 사용할지 결정
    4. train_test_split
    5. 머신러닝 model 처리


<딥러닝>
인공신경망(Artificial Neural Network)

퍼셉트론
        신경망의 노드, 단위 로직

    단층 퍼셉트론(Perceptron)
        순서
            input - Weights(가중치) - Net input func(시그마) - Activation func(시그마 sum 값을 활성화 함수에 대입) - output
        input data 수 = weights 수
        input data 와 weights 1:1 적용
        임계치(활성화되기 위한 최소값)보다 크면 1, 작으면 -1 출력
        XOR 문제를 학습할 수 없다 -> 범위안에 있는 data는 학습할 수 없다
        
        ex) 전기전자 : AND, OR, NOT 캐드

    다층 퍼셉트론(MLP)
        순서
            입력 -> 은닉층(h1) -> 은닉층2(h2) -> 출력 : 순방향
        input data와 은닉층 m:n 연결
        은닉층이 많으면 학습은 잘되지만 시간오래 걸리고 비용 많이 들어감

        -> 역전파로 다층 퍼셉트론 한계 극복

    역전파        
        순방향으로 학습한 output(실제값)과 예측값의 오차를 계산해 오차값을 다시 처음으로 보내주어 보정하고 다시 학습시키는 방법
        학습할수록 오차가 줄어듬
        오차는 파라미터 선정과 모델에 따라 양수, 음수, 절대값 등 기준이 달라질 수 있음

        역전파 hyper parameter => 많음
            hidden_layer_size = 은닉층 개수
            activation = 활성화 함수 지정
                logistic(시그모이드)
                relu(default값)
                tanh
            alpha = L2규제(Ridge : 제곱값, mse에 더해주는 베타값 살림) 적용
            batch_size = data size
            max_iter = 학습 반복 횟수

        신경망 복잡도 추정
            매개변수 = 은닉층 개수와 은닉층 유닛 수
            조정방법
                1) 과대적합
                2) 규제 강화를 위해 alhpa값을 증가시켜 일반화 성능을 향상시킴

    sigmoid func
        어떤 값 이전에는 0(False), 이후에는 1(True)을 반환

    ReLU func
        임계값까지는 0(False), 임계값을 넘으면 자신의 값을 가짐

    ELU
        0을 계산하기 때문에 잘못하면 0이나 무한대에서 나오지 못함
        OR연산, 나누기 0과 비슷

    Drop out
        신경망의 일부를 사용하지 않고 학습하는 방법
        input은 들어가지만 hidden layer 부분에서 빠졌다고 생각
        변수들을 하나씩 빼고 학습시킴. 의존도 감소시키기 위함

    permutation importance
        딥러닝은 중요 요소와 계산방법을 모름
        permutation importance를 통해 어떤일이 발생하는지를 추론할 수 있음. 요인을 제거했을 때 학습기능이 급격하게 떨어지는 요인을 찾을 수 있음

grid search
    hyper parameter를 대상으로 가능한 모든 조합을 시도하는 것
        -> dict 형태로 입력
    자동으로 최적의 hyper parameter를 찾아주는 모델
    gridsearchCV(모델, 모델의 파라미터 목록(dict 형태), CV)
    
    hyper parameter
        best_param : grid search 후에 찾은 최고의 파라미터 값
        best_score : 최고의 파라미터를사용한 교차 검증 점수
        best_estimator_ : 최고 성능의 모델


<기타>
코덱문제
    C++에서 발생하는 문제
    메모리에서 종료하는 코덱이 clear하게 반납이 안되고 조금 남아있기 때문에 다음에 들어온 코덱과 충돌이 일어남
        → 코덱을 바꿔야 함

기술의 특이점
    인공지능이 인공지능을 만들게 함 → 인간이 이해할 수 없는 기술이 나올거다 → 기술의 사용이 득인지 실인지 알 수 없음